# app.py â€” ëŒ€í‘œë‹˜ ì „ìš© ìœ íŠœë¸Œ ë¶„ì„ê¸° (ìµœì‹  ìµœì¢…ë³¸, ìš”ì²­ ìˆ˜ì • ë°˜ì˜)
# ê¸°ê°„(ì˜¬í•´/ì´ë²ˆë‹¬/ì´ë²ˆì£¼/ìµœê·¼7ì¼/ì˜¤ëŠ˜/ë¬´ì œí•œ), í™•ì¥ê²€ìƒ‰, í•œê¸€ì§€í‘œ,
# ì¸ë„¤ì¼ ì´ë¯¸ì§€ í™•ëŒ€(ì›ë³¸ë¹„ìœ¨), ì˜ìƒ ë§í¬, ë¯¸ë¦¬ë³´ê¸°, CSV, ìë§‰ ì¼ê´„ ìˆ˜ì§‘
# ì •ë ¬: ì¡°íšŒìˆ˜ / ìµœê·¼ ì—…ë¡œë“œ / ê´€ë ¨ì„±  (CII ë…¸ì¶œ/ì •ë ¬ ì œê±°)
# í‘œì—ì„œ "ì˜ìƒID" ì—´ ìˆ¨ê¹€ (ë‚´ë¶€ ì‚¬ìš©ë§Œ)

import os
import math
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Any, Tuple, Optional

import pandas as pd
import streamlit as st
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from youtube_transcript_api import (
    YouTubeTranscriptApi,
    TranscriptsDisabled,
    NoTranscriptFound,
)

# -----------------------------
# ê³µí†µ ìƒìˆ˜/ìœ í‹¸
# -----------------------------
KST = timezone(timedelta(hours=9))

def get_api_key() -> str:
    """Secrets/í™˜ê²½ë³€ìˆ˜ì—ì„œ YT_API_KEY ë˜ëŠ” YOUTUBE_API_KEY ìë™ ì¸ì‹"""
    key = None
    try:
        key = st.secrets.get("YT_API_KEY") or st.secrets.get("YOUTUBE_API_KEY")
    except Exception:
        key = None
    if not key:
        key = os.environ.get("YT_API_KEY") or os.environ.get("YOUTUBE_API_KEY")
    if not key:
        st.error(
            "API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.\n"
            "Streamlit Secretsì— `YT_API_KEY=\"...\"` ë˜ëŠ” `YOUTUBE_API_KEY=\"...\"` ì €ì¥í•˜ê±°ë‚˜\n"
            "í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì •í•´ ì£¼ì„¸ìš”."
        )
        st.stop()
    return key

def build_youtube():
    return build("youtube", "v3", developerKey=get_api_key())

def iso8601_to_seconds(iso: str) -> int:
    # 'PT1H2M10S' â†’ ì´ˆ
    h = m = s = 0
    if not iso:
        return 0
    t = iso.split("T")[-1]
    num = ""
    for ch in t:
        if ch.isdigit():
            num += ch
        else:
            if ch == "H" and num:
                h = int(num)
            if ch == "M" and num:
                m = int(num)
            if ch == "S" and num:
                s = int(num)
            num = ""
    return h * 3600 + m * 60 + s

def seconds_to_mmss(sec: int) -> str:
    return f"{sec // 60}:{sec % 60:02d}"

def safe_int(x) -> int:
    try:
        return int(x)
    except Exception:
        return 0

def calc_cii(row: pd.Series, now: datetime) -> float:
    """
    CII(ê°€ì¤‘ ë°˜ì‘ì§€í‘œ) ë‚´ë¶€ ê³„ì‚°(í˜„ì¬ëŠ” í™”ë©´ ë¯¸ë…¸ì¶œ)
      ì°¸ì—¬ë„ e = (ì¢‹ì•„ìš” + 3*ëŒ“ê¸€) / ì¡°íšŒìˆ˜
      ì±„ë„ê·œëª¨ b = log10(êµ¬ë…ì+1)
      ì‹ ì„ ë„ f = exp(-ì¼ìˆ˜/30)
    """
    views = max(1, safe_int(row.get("view_count", 0)))
    likes = safe_int(row.get("like_count", 0))
    comments = safe_int(row.get("comment_count", 0))
    e = (likes + 3 * comments) / views
    subs = safe_int(row.get("channel_subscribers", 0))
    b = math.log10(subs + 1) if subs >= 0 else 0.0
    published_at = row.get("published_at_dt", now)
    days = max(0.0, (now - published_at).total_seconds() / 86400.0)
    f = math.exp(-days / 30.0)
    return 100.0 * e * b * f

def pick_duration(label: str) -> str:
    return {"ì „ì²´": "any", "4ë¶„ ë¯¸ë§Œ": "short", "4~20ë¶„": "medium", "20ë¶„ ì´ìƒ": "long"}.get(label, "any")

def pick_order(label: str) -> str:
    # ê²€ìƒ‰ APIìš© ì •ë ¬ ë§¤í•‘(í™”ë©´ ì •ë ¬ì€ ì•„ë˜ì—ì„œ ë³„ë„ ì²˜ë¦¬)
    return {
        "ì¡°íšŒìˆ˜(ë‚´ë¦¼ì°¨ìˆœ)": "viewCount",
        "ìµœê·¼ ì—…ë¡œë“œ": "date",
        "ê´€ë ¨ì„±": "relevance",
    }.get(label, "viewCount")

def start_of_this_month_kst() -> datetime:
    now = datetime.now(KST)
    return datetime(now.year, now.month, 1, 0, 0, 0, tzinfo=KST)

def start_of_this_year_kst() -> datetime:
    now = datetime.now(KST)
    return datetime(now.year, 1, 1, 0, 0, 0, tzinfo=KST)

def start_of_today_kst() -> datetime:
    now = datetime.now(KST)
    return datetime(now.year, now.month, now.day, 0, 0, 0, tzinfo=KST)

def start_of_this_week_kst() -> datetime:
    # í•œêµ­ ê¸°ì¤€ ì›”ìš”ì¼ 00:00
    now = datetime.now(KST)
    monday = now - timedelta(days=now.weekday())
    return datetime(monday.year, monday.month, monday.day, 0, 0, 0, tzinfo=KST)

def to_rfc3339(dt: datetime) -> str:
    return dt.astimezone(timezone.utc).isoformat().replace("+00:00", "Z")

def now_utc_rfc3339() -> str:
    return datetime.now(timezone.utc).isoformat().replace("+00:00", "Z")

# í™•ì¥ ê²€ìƒ‰(ë‹¤ë³€í˜•)
def expand_queries(q: str) -> list[str]:
    q = q.strip()
    variants = [
        q,
        f"\"{q}\"",
        q.replace(" ", ""),
    ]
    tails = ["ì¶”ì²œ", "ê¿€í…œ", "ë¦¬ë·°", "í›„ê¸°", "TOP", "Best", "ëª¨ìŒ", "ê°€ì„±ë¹„", "ì‡¼ì¸ "]
    for t in tails:
        variants.append(f"{q} {t}")
    seen, uniq = set(), []
    for v in variants:
        if v not in seen:
            seen.add(v)
            uniq.append(v)
    return uniq

# -----------------------------
# API í˜¸ì¶œ
# -----------------------------
def search_video_ids(
    client, q: str, max_results: int, order: str,
    published_after: str, video_duration: str,
    published_before: Optional[str] = None
) -> List[str]:
    ids: List[str] = []
    page_token = None
    try:
        while len(ids) < max_results:
            req = client.search().list(
                q=q, part="id", type="video",
                maxResults=min(50, max_results - len(ids)),
                order=order,
                publishedAfter=published_after if published_after else None,
                publishedBefore=published_before if published_before else None,
                videoDuration=video_duration if video_duration != "any" else None,
                safeSearch="none",
            )
            if page_token:
                req.uri += f"&pageToken={page_token}"
            resp = req.execute()
            for item in resp.get("items", []):
                vid = item.get("id", {}).get("videoId")
                if vid:
                    ids.append(vid)
            page_token = resp.get("nextPageToken")
            if not page_token:
                break
    except HttpError as e:
        st.warning(f"Search API ì˜¤ë¥˜: {e}")
    return ids

def _best_thumb(snippet_thumbs: dict) -> str:
    # ì˜ë¦¼ ì—†ì´ í¬ê²Œ ë³´ê¸°: high â†’ medium â†’ default ìš°ì„  ì‚¬ìš©
    return (
        snippet_thumbs.get("high", {}).get("url")
        or snippet_thumbs.get("medium", {}).get("url")
        or snippet_thumbs.get("default", {}).get("url", "")
    )

def fetch_videos_and_channels(client, video_ids: List[str]) -> Tuple[pd.DataFrame, Dict[str, Dict[str, Any]]]:
    rows = []
    ch_ids = set()

    # videos
    for i in range(0, len(video_ids), 50):
        batch = video_ids[i:i+50]
        try:
            resp = client.videos().list(
                part="snippet,contentDetails,statistics",
                id=",".join(batch)
            ).execute()
        except HttpError as e:
            st.warning(f"Videos API ì˜¤ë¥˜: {e}")
            continue
        for item in resp.get("items", []):
            vid = item["id"]
            sn = item.get("snippet", {})
            stc = item.get("statistics", {})
            cd = item.get("contentDetails", {})
            ch_id = sn.get("channelId", "")
            ch_ids.add(ch_id)
            rows.append({
                "video_id": vid,
                "channel_id": ch_id,
                "channel_title": sn.get("channelTitle", ""),
                "title": sn.get("title", ""),
                "published_at": sn.get("publishedAt", ""),
                "thumbnail_url": _best_thumb(sn.get("thumbnails", {})),
                "duration_iso": cd.get("duration", "PT0S"),
                "view_count": safe_int(stc.get("viewCount", 0)),
                "like_count": safe_int(stc.get("likeCount", 0)),
                "comment_count": safe_int(stc.get("commentCount", 0)),
            })
    df = pd.DataFrame(rows)

    # channels (êµ¬ë…ì)
    channels: Dict[str, Dict[str, Any]] = {}
    if ch_ids:
        ids = list(ch_ids)
        for i in range(0, len(ids), 50):
            batch = ids[i:i+50]
            try:
                resp = client.channels().list(
                    part="statistics",
                    id=",".join(batch)
                ).execute()
            except HttpError as e:
                st.warning(f"Channels API ì˜¤ë¥˜: {e}")
                continue
            for item in resp.get("items", []):
                cid = item["id"]
                stc = item.get("statistics", {})
                channels[cid] = {"subscribers": safe_int(stc.get("subscriberCount", 0))}
    return df, channels

def enrich_dataframe(df: pd.DataFrame, channels: Dict[str, Dict[str, Any]]) -> pd.DataFrame:
    if df.empty:
        return df
    now = datetime.now(KST)
    df = df.copy()
    df["published_at_dt"] = pd.to_datetime(df["published_at"], errors="coerce")
    df["duration_sec"] = df["duration_iso"].apply(iso8601_to_seconds)
    df["duration_mmss"] = df["duration_sec"].apply(seconds_to_mmss)
    df["channel_subscribers"] = df["channel_id"].map(lambda x: channels.get(x, {}).get("subscribers", 0))
    # CIIëŠ” ë‚´ë¶€ë§Œ (ë…¸ì¶œ X)
    df["CII"] = df.apply(lambda r: calc_cii(r, now), axis=1)

    # ì±„ë„ ì ìœ ìœ¨(%)
    counts = df["channel_title"].value_counts(dropna=False)
    share = {k: (counts[k] / len(df)) * 100.0 for k in counts.index}
    df["channel_share_pct"] = df["channel_title"].map(lambda x: share.get(x, 0.0))
    return df

# -----------------------------
# UI
# -----------------------------
st.set_page_config(page_title="ëŒ€í‘œë‹˜ ì „ìš© ìœ íŠœë¸Œ ë¶„ì„ê¸°", layout="wide")
st.title("YouTube íƒìƒ‰ & ëŒ€ëŸ‰ ë¹„êµ (ìµœì¢…ë³¸)")
st.caption("ê¸°ê°„ í™•ì¥ + í™•ì¥ê²€ìƒ‰ + í•œê¸€ì§€í‘œ + ì¸ë„¤ì¼ í™•ëŒ€ + ë§í¬ + ë¯¸ë¦¬ë³´ê¸° + CSV + ìë§‰ ì¼ê´„ ìˆ˜ì§‘")

with st.sidebar:
    st.subheader("ê²€ìƒ‰ ì¡°ê±´")
    keyword = st.text_input("í‚¤ì›Œë“œ", value="ì¿ íŒ¡ ê¿€í…œ")

    uploaded_when = st.selectbox(
        "ì—…ë¡œë“œ ì‹œê¸°",
        ["ì˜¬í•´", "ì´ë²ˆë‹¬", "ì´ë²ˆì£¼", "ìµœê·¼ 7ì¼", "ì˜¤ëŠ˜", "ì œí•œ ì—†ìŒ"],
        index=1  # ê¸°ë³¸: ì´ë²ˆë‹¬
    )

    duration_label = st.selectbox("ì˜ìƒ ê¸¸ì´", ["ì „ì²´", "4ë¶„ ë¯¸ë§Œ", "4~20ë¶„", "20ë¶„ ì´ìƒ"], index=1)
    # ğŸ”½ ì •ë ¬ì—ì„œ 'CII' ì œê±°, 'ê´€ë ¨ì„±' ì¶”ê°€
    order = st.selectbox("ì •ë ¬", ["ì¡°íšŒìˆ˜(ë‚´ë¦¼ì°¨ìˆœ)", "ìµœê·¼ ì—…ë¡œë“œ", "ê´€ë ¨ì„±"], 0)

    # í™•ì¥ ê²€ìƒ‰(ë‹¤ë³€í˜•)
    expand_mode = st.checkbox("í™•ì¥ ê²€ìƒ‰(ë‹¤ë³€í˜•)", value=True, help="ë”°ì˜´í‘œ/ê³µë°±ì œê±°/ì—°ê´€ë‹¨ì–´ ì¡°í•©ìœ¼ë¡œ ë” ë§ì€ ì†ŒìŠ¤ ìˆ˜ì§‘")

    # ê²°ê³¼ ìˆ˜ ìƒí–¥
    max_results = st.slider("ìµœëŒ€ ê²°ê³¼ ìˆ˜", 10, 300, 100, 10)

    sample_take = st.number_input("í•œ ë²ˆì— ì„ íƒí•  ê°œìˆ˜", 1, 50, 5, 1)
    run = st.button("ê²€ìƒ‰ ì‹¤í–‰", type="primary")

client = build_youtube()

# === ë‚ ì§œ ë²”ìœ„ ê³„ì‚° ===
published_after = ""
published_before = now_utc_rfc3339()  # ìƒí•œì€ í˜„ì¬ ì‹œê°

if uploaded_when == "ì˜¬í•´":
    start_dt = start_of_this_year_kst()
    published_after = to_rfc3339(start_dt)
elif uploaded_when == "ì´ë²ˆë‹¬":
    start_dt = start_of_this_month_kst()
    published_after = to_rfc3339(start_dt)
elif uploaded_when == "ì´ë²ˆì£¼":
    start_dt = start_of_this_week_kst()
    published_after = to_rfc3339(start_dt)
elif uploaded_when == "ìµœê·¼ 7ì¼":
    start_dt = datetime.now(KST) - timedelta(days=7)
    published_after = to_rfc3339(start_dt)
elif uploaded_when == "ì˜¤ëŠ˜":
    start_dt = start_of_today_kst()
    published_after = to_rfc3339(start_dt)
else:  # ì œí•œ ì—†ìŒ
    published_after = ""
    published_before = None  # ìƒí•œ ë¯¸ì ìš©

# -----------------------------
# ì‹¤í–‰
# -----------------------------
if run:
    with st.spinner("ê²€ìƒ‰ ì¤‘..."):
        # í™•ì¥ ê²€ìƒ‰
        if expand_mode:
            queries = expand_queries(keyword)
            want = max_results
            ids_set: set[str] = set()
            per = max(10, want // max(1, len(queries)))  # ì¿¼ë¦¬ë‹¹ ìµœì†Œ 10ê°œ
            for qv in queries:
                if len(ids_set) >= want:
                    break
                got = search_video_ids(
                    client=client,
                    q=qv,
                    max_results=min(per, want - len(ids_set)),
                    order=pick_order(order),
                    published_after=published_after,
                    video_duration=pick_duration(duration_label),
                    published_before=published_before,
                )
                ids_set.update(got)
            ids = list(ids_set)[:want]
        else:
            ids = search_video_ids(
                client=client,
                q=keyword,
                max_results=max_results,
                order=pick_order(order),
                published_after=published_after,
                video_duration=pick_duration(duration_label),
                published_before=published_before,
            )

        if not ids:
            st.error("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ê±°ë‚˜ API ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. (API í‚¤/ê¶Œí•œ/ì¿¼í„°/í•„í„° í™•ì¸)")
            st.stop()

        df_raw, ch_map = fetch_videos_and_channels(client, ids)
        df = enrich_dataframe(df_raw, ch_map)

        # ===== ì§€í‘œ ì»¬ëŸ¼ ì¶”ê°€ (CIIëŠ” ë‚´ë¶€ë§Œ) =====
        if not df.empty:
            safe_views = df["view_count"].replace(0, 1)
            df["like_rate_pct"] = (df["like_count"] / safe_views * 100).round(2)          # ì¢‹ì•„ìš”/ì¡°íšŒìˆ˜(%)
            df["comment_per_mille"] = (df["comment_count"] / safe_views * 1000).round(2)  # ëŒ“ê¸€/ì¡°íšŒìˆ˜(â€°)
            df["channel_share_pct"] = df["channel_share_pct"].round(2)

        # ===== í™”ë©´ ì •ë ¬ =====
        if not df.empty:
            if order == "ì¡°íšŒìˆ˜(ë‚´ë¦¼ì°¨ìˆœ)":
                df = df.sort_values("view_count", ascending=False)
            elif order == "ìµœê·¼ ì—…ë¡œë“œ":
                df = df.sort_values("published_at_dt", ascending=False)
            # 'ê´€ë ¨ì„±'ì€ ê²€ìƒ‰ APIê°€ ë°˜í™˜í•œ ìˆœì„œë¥¼ ìœ ì§€ (ì¶”ê°€ ì†ŒíŠ¸ ì—†ìŒ)

        st.success(f"ê°€ì ¸ì˜¨ ì˜ìƒ {len(df)}ê°œ")

        if df.empty:
            st.warning("í‘œì‹œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            st.stop()

        # ===== í‘œì‹œìš© DF êµ¬ì„± =====
        show_cols = [
            "video_id","channel_title","title","published_at_dt",
            "channel_subscribers","view_count","like_count","comment_count",
            "like_rate_pct","comment_per_mille","duration_mmss",
            "channel_share_pct","thumbnail_url"
        ]
        nice_df = df[show_cols].copy()
        nice_df["ì˜ìƒ ë§í¬"] = "https://www.youtube.com/watch?v=" + nice_df["video_id"]
        nice_df["ì¸ë„¤ì¼"] = nice_df["thumbnail_url"]

        # í•œê¸€ ë¨¸ë¦¬ë§ë¡œ ë³€ê²½ (ì˜ìƒIDëŠ” ë‚´ë¶€ë§Œ ì‚¬ìš©)
        nice_df = nice_df.rename(columns={
            # "video_id":"ì˜ìƒID",  # âŒ í…Œì´ë¸” ë…¸ì¶œ ì œê±°
            "channel_title":"ì±„ë„ëª…",
            "title":"ì œëª©",
            "published_at_dt":"ê²Œì‹œì¼",
            "channel_subscribers":"êµ¬ë…ììˆ˜",
            "view_count":"ì¡°íšŒìˆ˜",
            "like_count":"ì¢‹ì•„ìš”ìˆ˜",
            "comment_count":"ëŒ“ê¸€ìˆ˜",
            "like_rate_pct":"ì¢‹ì•„ìš”/ì¡°íšŒìˆ˜(%)",
            "comment_per_mille":"ëŒ“ê¸€/ì¡°íšŒìˆ˜(â€°)",
            "duration_mmss":"ì˜ìƒ ê¸¸ì´",
            "channel_share_pct":"ì±„ë„ ì ìœ ìœ¨(%)",
        })

        # ë…¸ì¶œ ì»¬ëŸ¼(ì˜ìƒID, CII ë¯¸í¬í•¨)
        display_cols = [
            "ì¸ë„¤ì¼","ì˜ìƒ ë§í¬",
            "ì±„ë„ëª…","ì œëª©","ê²Œì‹œì¼",
            "êµ¬ë…ììˆ˜","ì¡°íšŒìˆ˜","ì¢‹ì•„ìš”ìˆ˜","ëŒ“ê¸€ìˆ˜",
            "ì¢‹ì•„ìš”/ì¡°íšŒìˆ˜(%)","ëŒ“ê¸€/ì¡°íšŒìˆ˜(â€°)","ì˜ìƒ ê¸¸ì´","ì±„ë„ ì ìœ ìœ¨(%)"
        ]

        st.dataframe(
            nice_df[display_cols],
            use_container_width=True,
            hide_index=True,
            column_config={
                # ì¸ë„¤ì¼ í¬ê²Œ(ì›ë³¸ ë¹„ìœ¨ ê·¸ëŒ€ë¡œ ë Œë”)
                "ì¸ë„¤ì¼": st.column_config.ImageColumn("ì¸ë„¤ì¼", width=180),
                "ì˜ìƒ ë§í¬": st.column_config.LinkColumn("ì˜ìƒ ë³´ê¸°", display_text="ì—´ê¸°"),
            },
        )

        # CSV ë‹¤ìš´ë¡œë“œ (ì—‘ì…€ í˜¸í™˜: utf-8-sig)
        csv = nice_df[display_cols].to_csv(index=False).encode("utf-8-sig")
        st.download_button("í˜„ì¬ ê²°ê³¼ CSV ë‹¤ìš´ë¡œë“œ", data=csv, file_name="youtube_results.csv")

        # ===== ë¯¸ë¦¬ë³´ê¸° =====
        st.markdown("---")
        st.subheader("ì˜ìƒ ë¯¸ë¦¬ë³´ê¸°")
        # ì˜ìƒIDëŠ” í‘œì— ë³´ì´ì§€ ì•Šì§€ë§Œ ì„ íƒ ë¬¸ìì—´ì—” í¬í•¨í•´ë„ ë¬´ë°©(í™•ì¸ ìš©)
        sel_options = (nice_df["ì œëª©"] + " | " + nice_df["ì±„ë„ëª…"] + " | " + df["video_id"]).tolist()
        sel = st.selectbox("ë¯¸ë¦¬ë³¼ ì˜ìƒ ì„ íƒ", options=["ì„ íƒ ì•ˆ í•¨"] + sel_options, index=0)
        if sel != "ì„ íƒ ì•ˆ í•¨":
            vid = sel.split("|")[-1].strip()
            st.video(f"https://www.youtube.com/watch?v={vid}")

        # ===== ìë§‰ ì¼ê´„ ìˆ˜ì§‘ / TXT ë‹¤ìš´ë¡œë“œ =====
        st.markdown("---")
        st.subheader("ì„ íƒí•œ ì˜ìƒ ìë§‰ ì¼ê´„ ìˆ˜ì§‘")
        options = df["video_id"].tolist()
        default_pick = options[:min(5, len(options))]
        selected_ids = st.multiselect("ìë§‰ì„ ìˆ˜ì§‘í•  ì˜ìƒ ì„ íƒ", options=options, default=default_pick)
        if st.button("ì„ íƒ ì˜ìƒ ìë§‰ ìˆ˜ì§‘ â†’ TXT ë‹¤ìš´ë¡œë“œ", type="primary"):
            texts = []
            for vid in selected_ids:
                try:
                    tr = YouTubeTranscriptApi.get_transcript(vid, languages=["ko","ko-KR","ko+en","en"])
                    text = "\n".join([x.get("text","") for x in tr if x.get("text")])
                except (TranscriptsDisabled, NoTranscriptFound):
                    text = ""
                except Exception:
                    text = ""
                texts.append({"video_id": vid, "caption": text})
            txt = ""
            for item in texts:
                txt += f"=== {item['video_id']} ===\n{item['caption']}\n\n"
            st.download_button("ìë§‰ TXT ë‹¤ìš´ë¡œë“œ", data=txt.encode("utf-8"), file_name="captions.txt")

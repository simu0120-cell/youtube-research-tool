# app.py â€” ìµœì†Œ ì¿¼í„°/ìµœì†Œ ë¹„ìš© ìµœì í™” ë²„ì „
# íŠ¹ì§•:
# - ì €ì¿¼í„° ëª¨ë“œ(ê¸°ë³¸ ON): í™•ì¥ê²€ìƒ‰ ìë™ OFF, ê²°ê³¼ 50ê°œ ìë™ ì œí•œ
# - ìºì‹œ(st.cache_data, 1ì‹œê°„): ê°™ì€ ì¡°ê±´ ì¬ê²€ìƒ‰ ì‹œ API í˜¸ì¶œ 0
# - í° ìº”ë²„ìŠ¤ í‘œ(ë‚´ë¶€ ìŠ¤í¬ë¡¤ ìµœì†Œí™”), CSV ë‹¤ìš´ë¡œë“œ
# - ì •ë ¬: ì¡°íšŒìˆ˜/ìµœê·¼ ì—…ë¡œë“œ/ê´€ë ¨ì„±
# - í•œê¸€ ì§€í‘œ + ì¸ë„¤ì¼(ê³ í™”ì§ˆ ìš°ì„ ) + ì˜ìƒ ë§í¬
# - quotaExceeded(403) í•œêµ­ì–´ ì¹œì ˆ ì•ˆë‚´

import os
from math import ceil
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Any, Tuple, Optional

import pandas as pd
import streamlit as st
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

# =========================
# ê³µí†µ/ìœ í‹¸
# =========================
KST = timezone(timedelta(hours=9))

def get_api_key() -> str:
    """Secrets/í™˜ê²½ë³€ìˆ˜ì—ì„œ YT_API_KEY ë˜ëŠ” YOUTUBE_API_KEY ìë™ ì¸ì‹"""
    key = None
    try:
        key = st.secrets.get("YT_API_KEY") or st.secrets.get("YOUTUBE_API_KEY")
    except Exception:
        key = None
    if not key:
        key = os.environ.get("YT_API_KEY") or os.environ.get("YOUTUBE_API_KEY")
    if not key:
        st.error(
            "API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.\n"
            "Streamlit Secretsì— `YT_API_KEY=\"...\"` ë˜ëŠ” `YOUTUBE_API_KEY=\"...\"` ì €ì¥í•˜ê±°ë‚˜\n"
            "í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì •í•´ ì£¼ì„¸ìš”."
        )
        st.stop()
    return key

def build_youtube():
    return build("youtube", "v3", developerKey=get_api_key())

def iso8601_to_seconds(iso: str) -> int:
    h = m = s = 0
    if not iso:
        return 0
    t = iso.split("T")[-1]
    num = ""
    for ch in t:
        if ch.isdigit():
            num += ch
        else:
            if ch == "H" and num:
                h = int(num)
            if ch == "M" and num:
                m = int(num)
            if ch == "S" and num:
                s = int(num)
            num = ""
    return h * 3600 + m * 60 + s

def seconds_to_mmss(sec: int) -> str:
    return f"{sec // 60}:{sec % 60:02d}"

def safe_int(x) -> int:
    try:
        return int(x)
    except Exception:
        return 0

def pick_duration(label: str) -> str:
    return {"ì „ì²´": "any", "4ë¶„ ë¯¸ë§Œ": "short", "4~20ë¶„": "medium", "20ë¶„ ì´ìƒ": "long"}.get(label, "any")

def pick_order(label: str) -> str:
    return {
        "ì¡°íšŒìˆ˜(ë‚´ë¦¼ì°¨ìˆœ)": "viewCount",
        "ìµœê·¼ ì—…ë¡œë“œ": "date",
        "ê´€ë ¨ì„±": "relevance",
    }.get(label, "viewCount")

def start_of_this_month_kst() -> datetime:
    now = datetime.now(KST)
    return datetime(now.year, now.month, 1, 0, 0, 0, tzinfo=KST)

def start_of_this_year_kst() -> datetime:
    now = datetime.now(KST)
    return datetime(now.year, 1, 1, 0, 0, 0, tzinfo=KST)

def start_of_today_kst() -> datetime:
    now = datetime.now(KST)
    return datetime(now.year, now.month, now.day, 0, 0, 0, tzinfo=KST)

def start_of_this_week_kst() -> datetime:
    now = datetime.now(KST)
    monday = now - timedelta(days=now.weekday())
    return datetime(monday.year, monday.month, monday.day, 0, 0, 0, tzinfo=KST)

def to_rfc3339(dt: datetime) -> str:
    return dt.astimezone(timezone.utc).isoformat().replace("+00:00", "Z")

def now_utc_rfc3339() -> str:
    return datetime.now(timezone.utc).isoformat().replace("+00:00", "Z")

def expand_queries(q: str) -> list[str]:
    """í™•ì¥ ê²€ìƒ‰(ë‹¤ë³€í˜•) â€” ì €ì¿¼í„° ëª¨ë“œ ê¸°ë³¸ OFF"""
    q = q.strip()
    variants = [q, f"\"{q}\"", q.replace(" ", "")]
    tails = ["ì¶”ì²œ", "ê¿€í…œ", "ë¦¬ë·°", "í›„ê¸°", "TOP", "Best", "ëª¨ìŒ", "ê°€ì„±ë¹„", "ì‡¼ì¸ "]
    for t in tails:
        variants.append(f"{q} {t}")
    seen, uniq = set(), []
    for v in variants:
        if v not in seen:
            seen.add(v)
            uniq.append(v)
    return uniq

# =========================
# API í˜¸ì¶œ
# =========================
def search_video_ids(
    client, q: str, max_results: int, order: str,
    published_after: str, video_duration: str,
    published_before: Optional[str] = None
) -> List[str]:
    ids: List[str] = []
    page_token = None
    try:
        while len(ids) < max_results:
            req = client.search().list(
                q=q, part="id", type="video",
                maxResults=min(50, max_results - len(ids)),
                order=order,
                publishedAfter=published_after if published_after else None,
                publishedBefore=published_before if published_before else None,
                videoDuration=video_duration if video_duration != "any" else None,
                safeSearch="none",
            )
            if page_token:
                req.uri += f"&pageToken={page_token}"
            resp = req.execute()
            for item in resp.get("items", []):
                vid = item.get("id", {}).get("videoId")
                if vid:
                    ids.append(vid)
            page_token = resp.get("nextPageToken")
            if not page_token:
                break
    except HttpError as e:
        st.warning(f"Search API ì˜¤ë¥˜: {e}")
    return ids

def _best_thumb(snippet_thumbs: dict) -> str:
    return (
        snippet_thumbs.get("high", {}).get("url")
        or snippet_thumbs.get("medium", {}).get("url")
        or snippet_thumbs.get("default", {}).get("url", "")
    )

def fetch_videos_and_channels(client, video_ids: List[str]) -> Tuple[pd.DataFrame, Dict[str, Dict[str, Any]]]:
    rows = []
    ch_ids = set()
    # videos
    for i in range(0, len(video_ids), 50):
        batch = video_ids[i:i+50]
        try:
            resp = client.videos().list(
                part="snippet,contentDetails,statistics",
                id=",".join(batch)
            ).execute()
        except HttpError as e:
            st.warning(f"Videos API ì˜¤ë¥˜: {e}")
            continue
        for item in resp.get("items", []):
            vid = item["id"]
            sn = item.get("snippet", {})
            stc = item.get("statistics", {})
            cd = item.get("contentDetails", {})
            ch_id = sn.get("channelId", "")
            ch_ids.add(ch_id)
            rows.append({
                "video_id": vid,
                "channel_id": ch_id,
                "channel_title": sn.get("channelTitle", ""),
                "title": sn.get("title", ""),
                "published_at": sn.get("publishedAt", ""),
                "thumbnail_url": _best_thumb(sn.get("thumbnails", {})),
                "duration_iso": cd.get("duration", "PT0S"),
                "view_count": safe_int(stc.get("viewCount", 0)),
                "like_count": safe_int(stc.get("likeCount", 0)),
                "comment_count": safe_int(stc.get("commentCount", 0)),
            })
    df = pd.DataFrame(rows)
    # channels(êµ¬ë…ì)
    channels: Dict[str, Dict[str, Any]] = {}
    if ch_ids:
        ids = list(ch_ids)
        for i in range(0, len(ids), 50):
            batch = ids[i:i+50]
            try:
                resp = client.channels().list(
                    part="statistics",
                    id=",".join(batch)
                ).execute()
            except HttpError as e:
                st.warning(f"Channels API ì˜¤ë¥˜: {e}")
                continue
            for item in resp.get("items", []):
                cid = item["id"]
                stc = item.get("statistics", {})
                channels[cid] = {"subscribers": safe_int(stc.get("subscriberCount", 0))}
    return df, channels

def enrich_dataframe(df: pd.DataFrame, channels: Dict[str, Dict[str, Any]]) -> pd.DataFrame:
    if df.empty:
        return df
    df = df.copy()
    df["published_at_dt"] = pd.to_datetime(df["published_at"], errors="coerce")
    df["duration_sec"] = df["duration_iso"].apply(iso8601_to_seconds)
    df["duration_mmss"] = df["duration_sec"].apply(seconds_to_mmss)
    df["channel_subscribers"] = df["channel_id"].map(lambda x: channels.get(x, {}).get("subscribers", 0))
    # ì±„ë„ ì ìœ ìœ¨(%)
    counts = df["channel_title"].value_counts(dropna=False)
    share = {k: (counts[k] / len(df)) * 100.0 for k in counts.index}
    df["channel_share_pct"] = df["channel_title"].map(lambda x: share.get(x, 0.0))
    return df

# =========================
# ìºì‹œ(1ì‹œê°„)
# =========================
@st.cache_data(ttl=3600)
def cached_search_ids(q, max_results, order, published_after, video_duration, published_before):
    client = build_youtube()
    return search_video_ids(client, q, max_results, order, published_after, video_duration, published_before)

@st.cache_data(ttl=3600)
def cached_fetch_videos_and_channels(video_ids):
    client = build_youtube()
    return fetch_videos_and_channels(client, video_ids)

def estimate_quota_cost(num_queries:int, want:int, ch_count:int) -> int:
    """ëŒ€ëµì ì¸ ìœ ë‹› ì¶”ì •(ê° ì¡ê¸°ìš©)
       search.list: 100ìœ ë‹›/í˜¸ì¶œ(ìµœëŒ€ 50ê°œ/í˜ì´ì§€)
       videos.list: 1ìœ ë‹›/í˜¸ì¶œ(ìµœëŒ€ 50ê°œ)
       channels.list: 1ìœ ë‹›/í˜¸ì¶œ(ìµœëŒ€ 50ê°œ)
    """
    search_pages = num_queries * ceil(max(1, want)/50)
    video_calls  = ceil(max(1, want)/50)
    ch_calls     = ceil(max(1, ch_count)/50)
    return 100*search_pages + video_calls + ch_calls

# =========================
# UI
# =========================
st.set_page_config(page_title="ëŒ€í‘œë‹˜ ì „ìš© ìœ íŠœë¸Œ ë¶„ì„ê¸°", layout="wide")
st.title("YouTube íƒìƒ‰ & ëŒ€ëŸ‰ ë¹„êµ (ìµœì € ì¿¼í„°)")
st.caption("ìºì‹œ + ì €ì¿¼í„° ëª¨ë“œ + í° ìº”ë²„ìŠ¤ í‘œ | í•œê¸€ ì§€í‘œ + ì¸ë„¤ì¼ + ë§í¬ + CSV")

with st.sidebar:
    st.subheader("ê²€ìƒ‰ ì¡°ê±´")
    keyword = st.text_input("í‚¤ì›Œë“œ", value="ì¿ íŒ¡ ê¿€í…œ")

    uploaded_when = st.selectbox(
        "ì—…ë¡œë“œ ì‹œê¸°",
        ["ì˜¬í•´", "ì´ë²ˆë‹¬", "ì´ë²ˆì£¼", "ìµœê·¼ 7ì¼", "ì˜¤ëŠ˜", "ì œí•œ ì—†ìŒ"],
        index=1
    )

    duration_label = st.selectbox("ì˜ìƒ ê¸¸ì´", ["ì „ì²´", "4ë¶„ ë¯¸ë§Œ", "4~20ë¶„", "20ë¶„ ì´ìƒ"], index=1)
    order = st.selectbox("ì •ë ¬", ["ì¡°íšŒìˆ˜(ë‚´ë¦¼ì°¨ìˆœ)", "ìµœê·¼ ì—…ë¡œë“œ", "ê´€ë ¨ì„±"], 0)

    # âœ… ê¸°ë³¸ê°’ì´ 'ì ˆì•½'ì´ ë˜ë„ë¡ True
    low_quota_mode = st.toggle("ì €ì¿¼í„° ëª¨ë“œ(ì¿¼í„° ì ˆì•½)", value=True,
                               help="í™•ì¥ê²€ìƒ‰ OFF + ê²°ê³¼ ìë™ ì œí•œ(â‰¤50) + í˜¸ì¶œ ìµœì†Œí™”")
    expand_mode = st.checkbox("í™•ì¥ ê²€ìƒ‰(ë‹¤ë³€í˜•)", value=False if low_quota_mode else True)

    # ì €ì¿¼í„° ëª¨ë“œë©´ UI ìƒì˜ ìµœëŒ€ ê²°ê³¼ ìˆ˜ëŠ” ë¬´ì‹œí•˜ê³  50ìœ¼ë¡œ ì œí•œë¨
    max_results = st.slider("ìµœëŒ€ ê²°ê³¼ ìˆ˜", 10, 300, 100, 10)

    run = st.button("ê²€ìƒ‰ ì‹¤í–‰", type="primary")

# ë‚ ì§œ ë²”ìœ„ ê³„ì‚°
published_after = ""
published_before = now_utc_rfc3339()
if uploaded_when == "ì˜¬í•´":
    published_after = to_rfc3339(start_of_this_year_kst())
elif uploaded_when == "ì´ë²ˆë‹¬":
    published_after = to_rfc3339(start_of_this_month_kst())
elif uploaded_when == "ì´ë²ˆì£¼":
    published_after = to_rfc3339(start_of_this_week_kst())
elif uploaded_when == "ìµœê·¼ 7ì¼":
    published_after = to_rfc3339(datetime.now(KST) - timedelta(days=7))
elif uploaded_when == "ì˜¤ëŠ˜":
    published_after = to_rfc3339(start_of_today_kst())
else:
    published_after = ""
    published_before = None

# =========================
# RUN
# =========================
if run:
    with st.spinner("ê²€ìƒ‰ ì¤‘..."):
        want = max_results
        use_expand = expand_mode
        if low_quota_mode:
            use_expand = False
            want = min(want, 50)
            st.info("ì €ì¿¼í„° ëª¨ë“œ: í™•ì¥ê²€ìƒ‰ OFF, ê²°ê³¼ ìˆ˜ 50ìœ¼ë¡œ ì œí•œí•©ë‹ˆë‹¤.")

        try:
            if use_expand:
                queries = expand_queries(keyword)
                ids_set = set()
                per = max(10, want // max(1, len(queries)))
                for qv in queries:
                    if len(ids_set) >= want:
                        break
                    got = cached_search_ids(
                        qv, min(per, want - len(ids_set)),
                        pick_order(order),
                        published_after,
                        pick_duration(duration_label),
                        published_before
                    )
                    ids_set.update(got)
                ids = list(ids_set)[:want]
                num_queries = len(queries)
            else:
                ids = cached_search_ids(
                    keyword, want,
                    pick_order(order),
                    published_after,
                    pick_duration(duration_label),
                    published_before
                )
                num_queries = 1

            if not ids:
                st.error("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ê±°ë‚˜ API ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
                st.stop()

            # ëŒ€ëµ ë¹„ìš© ì¶”ì •(ì°¸ê³ ìš©)
            rough_cost = estimate_quota_cost(num_queries, len(ids), ch_count=50)
            st.caption(f"ì˜ˆìƒ ì¿¼í„° ì‚¬ìš©(ëŒ€ëµ): ì•½ {rough_cost} ìœ ë‹›")

            df_raw, ch_map = cached_fetch_videos_and_channels(ids)

        except HttpError as e:
            if "quotaExceeded" in str(e):
                st.error(
                    "YouTube Data API ì¿¼í„°ê°€ ì†Œì§„ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
                    "ğŸ›  í•´ê²° ê°€ì´ë“œ:\n"
                    " â€¢ â€˜ì €ì¿¼í„° ëª¨ë“œâ€™ ì¼œê¸°(ê¶Œì¥)\n"
                    " â€¢ â€˜í™•ì¥ ê²€ìƒ‰â€™ ë„ê¸°\n"
                    " â€¢ â€˜ìµœëŒ€ ê²°ê³¼ ìˆ˜â€™ë¥¼ 30~50ìœ¼ë¡œ ë‚®ì¶”ê¸°\n"
                    " â€¢ â€˜ì—…ë¡œë“œ ì‹œê¸°â€™ë¥¼ â€˜ìµœê·¼ 7ì¼/ì˜¤ëŠ˜â€™ë¡œ ì¢íˆê¸°\n"
                    " â€¢ (ê°€ëŠ¥í•˜ë©´) ë‹¤ë¥¸ í”„ë¡œì íŠ¸ì˜ API í‚¤ ì‚¬ìš© ë˜ëŠ” ì¿¼í„° ì¦ì„¤"
                )
                st.stop()
            else:
                st.error(f"API ì˜¤ë¥˜: {e}")
                st.stop()

        # ê°€ê³µ
        df = enrich_dataframe(df_raw, ch_map)
        if df.empty:
            st.warning("í‘œì‹œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            st.stop()

        # í•œê¸€ ì§€í‘œ
        safe_views = df["view_count"].replace(0, 1)
        df["like_rate_pct"] = (df["like_count"] / safe_views * 100).round(2)
        df["comment_per_mille"] = (df["comment_count"] / safe_views * 1000).round(2)
        df["channel_share_pct"] = df["channel_share_pct"].round(2)

        # í™”ë©´ ì •ë ¬
        if order == "ì¡°íšŒìˆ˜(ë‚´ë¦¼ì°¨ìˆœ)":
            df = df.sort_values("view_count", ascending=False)
        elif order == "ìµœê·¼ ì—…ë¡œë“œ":
            df = df.sort_values("published_at_dt", ascending=False)
        # 'ê´€ë ¨ì„±'ì€ API ìˆœì„œ ìœ ì§€

        st.success(f"ê°€ì ¸ì˜¨ ì˜ìƒ {len(df)}ê°œ")

        # í‘œì‹œìš© DF
        show_cols = [
            "video_id","channel_title","title","published_at_dt",
            "channel_subscribers","view_count","like_count","comment_count",
            "like_rate_pct","comment_per_mille","duration_mmss",
            "channel_share_pct","thumbnail_url"
        ]
        nice_df = df[show_cols].copy()
        nice_df["ì˜ìƒ ë§í¬"] = "https://www.youtube.com/watch?v=" + nice_df["video_id"]
        nice_df["ì¸ë„¤ì¼"] = nice_df["thumbnail_url"]
        nice_df = nice_df.rename(columns={
            "channel_title":"ì±„ë„ëª…",
            "title":"ì œëª©",
            "published_at_dt":"ê²Œì‹œì¼",
            "channel_subscribers":"êµ¬ë…ììˆ˜",
            "view_count":"ì¡°íšŒìˆ˜",
            "like_count":"ì¢‹ì•„ìš”ìˆ˜",
            "comment_count":"ëŒ“ê¸€ìˆ˜",
            "like_rate_pct":"ì¢‹ì•„ìš”/ì¡°íšŒìˆ˜(%)",
            "comment_per_mille":"ëŒ“ê¸€/ì¡°íšŒìˆ˜(â€°)",
            "duration_mmss":"ì˜ìƒ ê¸¸ì´",
            "channel_share_pct":"ì±„ë„ ì ìœ ìœ¨(%)",
        })

        display_cols = [
            "ì¸ë„¤ì¼","ì˜ìƒ ë§í¬",
            "ì±„ë„ëª…","ì œëª©","ê²Œì‹œì¼",
            "êµ¬ë…ììˆ˜","ì¡°íšŒìˆ˜","ì¢‹ì•„ìš”ìˆ˜","ëŒ“ê¸€ìˆ˜",
            "ì¢‹ì•„ìš”/ì¡°íšŒìˆ˜(%)","ëŒ“ê¸€/ì¡°íšŒìˆ˜(â€°)","ì˜ìƒ ê¸¸ì´","ì±„ë„ ì ìœ ìœ¨(%)"
        ]

        # í° ìº”ë²„ìŠ¤(í˜ì´ì§€ ìŠ¤í¬ë¡¤ ì¤‘ì‹¬)
        height = int(min(1800, 46 * (len(nice_df) + 2)))
        if height < 600:
            height = 600

        st.dataframe(
            nice_df[display_cols],
            use_container_width=True,
            hide_index=True,
            height=height,
            column_config={
                "ì¸ë„¤ì¼": st.column_config.ImageColumn("ì¸ë„¤ì¼", width=180),
                "ì˜ìƒ ë§í¬": st.column_config.LinkColumn("ì˜ìƒ ë³´ê¸°", display_text="ì—´ê¸°"),
            },
        )

        # CSV
        csv = nice_df[display_cols].to_csv(index=False).encode("utf-8-sig")
        st.download_button("í˜„ì¬ ê²°ê³¼ CSV ë‹¤ìš´ë¡œë“œ", data=csv, file_name="youtube_results.csv")
